{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_Gradient_Check.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UgEc9RYwdNyS"},"source":["## Import all the necessary libraries"]},{"cell_type":"markdown","metadata":{"id":"UcVJ89lWfBqk"},"source":["Source: https://github.com/akshat57/Blind-Descent/blob/main/Blind_Descent-1-CNN.ipynb"]},{"cell_type":"code","metadata":{"id":"T5oQ47tOdNyU","executionInfo":{"status":"ok","timestamp":1611975282848,"user_tz":300,"elapsed":394,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["import numpy as np\n","import torch\n","import sys\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils import data\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torchvision.datasets import CIFAR10\n","\n","import matplotlib.pyplot as plt\n","import time\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","cuda = torch.cuda.is_available()\n","cuda = False"],"execution_count":89,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2ZlVqRvdNyV"},"source":["## Download the MNIST and CIFAR10 datasets"]},{"cell_type":"code","metadata":{"id":"snx5udk9dNyW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611975285196,"user_tz":300,"elapsed":2734,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}},"outputId":"80f7f641-a9f0-4c8b-dc81-2d0d1c5296a9"},"source":["train = MNIST('./MNIST_data', train=True, download=True, transform=transforms.ToTensor())\n","test = MNIST('./MNIST_data', train=False, download=True, transform=transforms.ToTensor())\n","train_MNIST_data = train.data; train_MNIST_labels = train.targets\n","test_MNIST_data = test.data; test_MNIST_labels = test.targets\n","\n","train = CIFAR10('./CIFAR10_data', train=True, download=True, transform=transforms.ToTensor())\n","test = CIFAR10('./CIFAR10_data', train=False, download=True, transform=transforms.ToTensor())\n","train_CIFAR10_data = train.data; train_CIFAR10_labels = train.targets\n","test_CIFAR10_data = test.data; test_CIFAR10_labels = test.targets\n","\n","print()\n","print(\"MNIST is already an array\")\n","print(train_MNIST_data.shape, train_MNIST_labels.shape, test_MNIST_data.shape, test_MNIST_labels.shape)\n","print()\n","print(\"CIFAR10 is a list of arrays\")\n","print(len(train_CIFAR10_data), len(train_CIFAR10_labels), len(test_CIFAR10_data), len(test_CIFAR10_labels))\n","print(train_CIFAR10_data[0].shape, test_CIFAR10_data[0].shape)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","\n","MNIST is already an array\n","torch.Size([60000, 28, 28]) torch.Size([60000]) torch.Size([10000, 28, 28]) torch.Size([10000])\n","\n","CIFAR10 is a list of arrays\n","50000 50000 10000 10000\n","(32, 32, 3) (32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g3JjIF9UdNyW"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2P-GNkhPdNyX","executionInfo":{"status":"ok","timestamp":1611975285197,"user_tz":300,"elapsed":2730,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["class CIFAR10Dataset(data.Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.Y)\n","\n","    def __getitem__(self,index):\n","        X = np.transpose(self.X[index], (2, 0, 1))\n","        X = X.astype(float)\n","        Y = self.Y[index]\n","        return X,Y\n","\n","class MNIST_Dataset(data.Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.Y)\n","\n","    def __getitem__(self,index):\n","        X = np.pad(self.X[index], 2)\n","        X = np.repeat(X[:, :, np.newaxis], 3, axis = 2)\n","        X = np.transpose(X, (2, 0, 1))\n","        X = X.astype(float)\n","        Y = self.Y[index]\n","        return X,Y"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiFk7X_edNyZ"},"source":["Using the torch.utils.data DataLoader, we shuffle the data and set the batch size"]},{"cell_type":"code","metadata":{"id":"R-RXL-JJdNyZ","executionInfo":{"status":"ok","timestamp":1611975285199,"user_tz":300,"elapsed":2727,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["num_workers = 8 if cuda else 0 \n","batch_size = 64\n","    \n","# MNIST Training\n","train_dataset = MNIST_Dataset(train_MNIST_data, train_MNIST_labels)\n","\n","train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n","                    else dict(shuffle=True, batch_size=batch_size)\n","train_MNIST_loader = data.DataLoader(train_dataset, **train_loader_args)\n","\n","# MNIST Testing\n","test_dataset = MNIST_Dataset(test_MNIST_data, test_MNIST_labels)\n","\n","test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n","                    else dict(shuffle=False, batch_size=1)\n","test_MNIST_loader = data.DataLoader(test_dataset, **test_loader_args)\n","\n","# CIFAR10 Training\n","train_dataset = CIFAR10Dataset(train_CIFAR10_data, train_CIFAR10_labels)\n","\n","train_loader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n","                    else dict(shuffle=True, batch_size=batch_size)\n","train_CIFAR10_loader = data.DataLoader(train_dataset, **train_loader_args)\n","\n","# CIFAR10 Testing\n","test_dataset = CIFAR10Dataset(test_CIFAR10_data, test_CIFAR10_labels)\n","\n","test_loader_args = dict(shuffle=False, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda\\\n","                    else dict(shuffle=False, batch_size=1)\n","test_CIFAR10_loader = data.DataLoader(test_dataset, **test_loader_args)"],"execution_count":92,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tlhg9dfadNyZ"},"source":["## Define our Neural Network Model \n","We define our model using the torch.nn.Module class"]},{"cell_type":"code","metadata":{"id":"1rWxDIX7dNyZ","executionInfo":{"status":"ok","timestamp":1611975285201,"user_tz":300,"elapsed":2725,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["class MyCNN_Model(nn.Module):\n","    def __init__(self):\n","        super(MyCNN_Model, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size = 5)\n","        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size = 5)\n","        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n","        self.conv3 = nn.Conv2d(32, 10, kernel_size = 5)\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.pool1(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool2(x)\n","        x = self.conv3(x)\n","        x = x.view(-1, 10)\n","        \n","        return x"],"execution_count":93,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVzzuB_IdNya"},"source":["## Create the model and define the Loss and Optimizer"]},{"cell_type":"code","metadata":{"id":"SgGr79Z6dNyc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611975285203,"user_tz":300,"elapsed":2723,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}},"outputId":"941f3979-56fc-4fe3-a915-7a7348500c5e"},"source":["criterion = nn.CrossEntropyLoss()\n","device = torch.device(\"cuda\" if cuda else \"cpu\")\n","model = MyCNN_Model()\n","model.to(device)\n","print(model)"],"execution_count":94,"outputs":[{"output_type":"stream","text":["MyCNN_Model(\n","  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(32, 10, kernel_size=(5, 5), stride=(1, 1))\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F5gYYVQidNyc","executionInfo":{"status":"ok","timestamp":1611975285367,"user_tz":300,"elapsed":2881,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["def train_epoch(model, train_loader, criterion):\n","    model.train()\n","\n","    running_loss = 0.0\n","    predictions = []\n","    ground_truth = []\n","    loss_den = 1\n","    \n","    start_time = time.time()\n","    for batch_idx, (data, target) in enumerate(train_loader):   \n","        # lr = np.power(10, np.random.uniform(-6, 1))\n","        lr = 0.001\n","\n","        data = data.to(device)\n","        target = target.to(device)\n","    \n","        #previous model\n","        outputs = model(data.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions = target.size(0)\n","        correct_predictions = (predicted == target).sum().item()\n","        acc = (correct_predictions/total_predictions)*100.0\n","        \n","        loss = criterion(outputs, target)\n","        loss.backward()\n","\n","        with torch.no_grad():\n","            model.conv1.weight -= (lr * model.conv1.weight.grad).float()\n","            model.conv2.weight -= (lr * model.conv2.weight.grad).float()\n","            model.conv3.weight -= (lr * model.conv3.weight.grad).float()\n","\n","        outputs = model(data.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_predictions = target.size(0)\n","        correct_predictions = (predicted == target).sum().item()\n","        acc_new = (correct_predictions/total_predictions)*100.0\n","        \n","        loss_new = criterion(outputs, target)\n","        loss_den += 1\n","\n","        #calculuating confusion matrix\n","        predictions += list(predicted.detach().cpu().numpy())\n","        ground_truth += list(target.detach().cpu().numpy())\n","\n","        '''\n","        if loss_new.item() > loss.item():\n","            model.conv1.weight += lr * model.conv1.weight.grad\n","            model.conv2.weight += lr * model.conv2.weight.grad\n","            model.conv3.weight += lr * model.conv3.weight.grad\n","\n","            running_loss += loss.item()\n","        else:\n","            running_loss += loss_new.item()\n","        '''\n","\n","        model.conv1.weight.grad.zero_()\n","        model.conv2.weight.grad.zero_()\n","        model.conv3.weight.grad.zero_()\n","        \n","    end_time = time.time()\n","\n","    running_loss /= loss_den\n","    \n","    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n","    \n","    return running_loss, model"],"execution_count":95,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5bSz-zcdNyd"},"source":["## Create a function that will evaluate our network's performance on the test set"]},{"cell_type":"code","metadata":{"id":"rAhuZ7uMdNyd","executionInfo":{"status":"ok","timestamp":1611975285370,"user_tz":300,"elapsed":2879,"user":{"displayName":"Prasad Narahari Raghavendra","photoUrl":"","userId":"13004936298388256166"}}},"source":["def test_model(model, test_loader, criterion):\n","    with torch.no_grad():\n","        model.eval()\n","\n","        running_loss = 0.0\n","        total_predictions = 0.0\n","        correct_predictions = 0.0\n","        \n","        predictions = []\n","        ground_truth = []\n","\n","        for batch_idx, (data, target) in enumerate(test_loader):   \n","            data = data.to(device)\n","            target = target.to(device)\n","\n","            outputs = model(data.float())\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_predictions += target.size(0)\n","            correct_predictions += (predicted == target).sum().item()\n","\n","            loss = criterion(outputs, target).detach()\n","            running_loss += loss.item()\n","            \n","            #calculuating confusion matrix\n","            predictions += list(predicted.detach().cpu().numpy())\n","            ground_truth += list(target.detach().cpu().numpy())\n","        \n","        #write_confusion_matrix('Testing', ground_truth, predictions)\n","        running_loss /= len(test_loader)\n","        acc = (correct_predictions/total_predictions)*100.0\n","        print('Testing Loss: ', running_loss)\n","        print('Testing Accuracy: ', acc, '%')\n","        return running_loss, acc\n"],"execution_count":96,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sF9evUwmdNye"},"source":["## Train the model for N epochs\n","We call our training and testing functions in a loop, while keeping track of the losses and accuracy. "]},{"cell_type":"code","metadata":{"id":"cjbgMQIFdNye","scrolled":true},"source":["n_epochs = 40\n","\n","model = MyCNN_Model(); model.to(device)\n","for i in range(n_epochs):\n","    train_loss, model = train_epoch(model, train_CIFAR10_loader, criterion)\n","    test_loss, CIFAR10_test_acc = test_model(model, test_CIFAR10_loader, criterion)\n","    print('='*20)\n","\n","model = MyCNN_Model(); model.to(device)\n","for i in range(n_epochs):\n","    train_loss, model = train_epoch(model, train_MNIST_loader, criterion)\n","    test_loss, MNIST_test_acc = test_model(model, test_MNIST_loader, criterion)\n","    print('='*20)"],"execution_count":null,"outputs":[]}]}